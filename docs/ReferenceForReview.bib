
@article{thomas_tensor_2018,
	title = {Tensor field networks: {Rotation}- and translation-equivariant neural networks for {3D} point clouds},
	shorttitle = {Tensor field networks},
	url = {http://arxiv.org/abs/1802.08219},
	abstract = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
	language = {en},
	urldate = {2021-11-30},
	journal = {arXiv:1802.08219 [cs]},
	author = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
	month = may,
	year = {2018},
	note = {arXiv: 1802.08219},
	keywords = {Network on points, Tensor Neural Network},
	file = {Thomas et al. - 2018 - Tensor field networks Rotation- and translation-e.pdf:C\:\\wenhao\\zotero\\storage\\NXNZFXLF\\Thomas et al. - 2018 - Tensor field networks Rotation- and translation-e.pdf:application/pdf},
}

@article{chen_graph_2019,
	title = {Graph {Networks} as a {Universal} {Machine} {Learning} {Framework} for {Molecules} and {Crystals}},
	volume = {31},
	issn = {0897-4756, 1520-5002},
	url = {https://pubs.acs.org/doi/10.1021/acs.chemmater.9b01294},
	doi = {10.1021/acs.chemmater.9b01294},
	abstract = {Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on \~{}60 000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps, and elastic moduli of crystals, achieving better than density functional theory accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy, and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure, and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).},
	language = {en},
	number = {9},
	urldate = {2021-11-30},
	journal = {Chemistry of Materials},
	author = {Chen, Chi and Ye, Weike and Zuo, Yunxing and Zheng, Chen and Ong, Shyue Ping},
	month = may,
	year = {2019},
	keywords = {Crystal Graph Networks},
	pages = {3564--3572},
	file = {Chen et al. - 2019 - Graph Networks as a Universal Machine Learning Fra.pdf:C\:\\wenhao\\zotero\\storage\\JMFN6XVQ\\Chen et al. - 2019 - Graph Networks as a Universal Machine Learning Fra.pdf:application/pdf},
}

@article{schutt_quantum-chemical_2017,
	title = {Quantum-chemical insights from deep tensor neural networks},
	volume = {8},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/ncomms13890},
	doi = {10.1038/ncomms13890},
	language = {en},
	number = {1},
	urldate = {2021-12-17},
	journal = {Nature Communications},
	author = {Sch{\"u}tt, Kristof T. and Arbabzadah, Farhad and Chmiela, Stefan and M{\"u}ller, Klaus R. and Tkatchenko, Alexandre},
	month = apr,
	year = {2017},
	keywords = {Molecular, Tensor neural network},
	pages = {13890},
	file = {Sch{\"u}tt et al. - 2017 - Quantum-chemical insights from deep tensor neural .pdf:C\:\\wenhao\\zotero\\storage\\IK8W7WPM\\Sch{\"u}tt et al. - 2017 - Quantum-chemical insights from deep tensor neural .pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
}

@book{sakurai_modern_1994,
	address = {Reading, Mass},
	edition = {Rev. ed},
	title = {Modern quantum mechanics},
	isbn = {978-0-201-53929-5},
	publisher = {Addison-Wesley Pub. Co},
	author = {Sakurai, J. J. and Tuan, San Fu},
	year = {1994},
	keywords = {Quantum theory},
}

@article{kondor_clebschgordan_2018,
	title = {Clebsch{\textendash}{Gordan} {Nets}: a {Fully} {Fourier} {Space} {Spherical} {Convolutional} {Neural} {Network}},
	abstract = {Recent work by Cohen et al. [1] has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch{\textendash}Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.},
	language = {en},
	journal = {32nd Conference on Neural Information Processing Systems},
	author = {Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
	year = {2018},
	keywords = {Convolution network, Spherical images},
	pages = {10},
	file = {Kondor et al. - Clebsch{\textendash}Gordan Nets a Fully Fourier Space Spheric.pdf:C\:\\wenhao\\zotero\\storage\\ZJ74P2Z5\\Kondor et al. - Clebsch{\textendash}Gordan Nets a Fully Fourier Space Spheric.pdf:application/pdf},
}

@article{kauderer-abrams_quantifying_2017,
	title = {Quantifying {Translation}-{Invariance} in {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1801.01450},
	abstract = {A fundamental problem in object recognition is the development of image representations that are invariant to common transformations such as translation, rotation, and small deformations. There are multiple hypotheses regarding the source of translation invariance in CNNs. One idea is that translation invariance is due to the increasing receptive field size of neurons in successive convolution layers. Another possibility is that invariance is due to the pooling operation. We develop a simple a tool, the translation-sensitivity map, which we use to visualize and quantify the translation-invariance of various architectures. We obtain the surprising result that architectural choices such as the number of pooling layers and the convolution filter size have only a secondary effect on the translationinvariance of a network. Our analysis identifies training data augmentation as the most important factor in obtaining translation-invariant representations of images using convolutional neural networks.},
	language = {en},
	urldate = {2022-02-02},
	journal = {arXiv:1801.01450 [cs]},
	author = {Kauderer-Abrams, Eric},
	month = dec,
	year = {2017},
	note = {arXiv: 1801.01450},
	keywords = {Convolution network},
	file = {Kauderer-Abrams - 2017 - Quantifying Translation-Invariance in Convolutiona.pdf:C\:\\wenhao\\zotero\\storage\\7JZQIQ2E\\Kauderer-Abrams - 2017 - Quantifying Translation-Invariance in Convolutiona.pdf:application/pdf},
}

@article{cohen_spherical_2018,
	title = {Spherical {CNNs}},
	url = {http://arxiv.org/abs/1801.10130},
	abstract = {Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.},
	language = {en},
	urldate = {2022-02-02},
	journal = {arXiv:1801.10130 [cs, stat]},
	author = {Cohen, Taco S. and Geiger, Mario and Koehler, Jonas and Welling, Max},
	month = feb,
	year = {2018},
	note = {arXiv: 1801.10130},
	keywords = {Convolution network, Spherical images},
	file = {Cohen et al. - 2018 - Spherical CNNs.pdf:C\:\\wenhao\\zotero\\storage\\7DP6I3AT\\Cohen et al. - 2018 - Spherical CNNs.pdf:application/pdf},
}

@article{cohen_steerable_2016,
	title = {Steerable {CNNs}},
	url = {http://arxiv.org/abs/1612.08498},
	abstract = {It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.},
	language = {en},
	urldate = {2022-02-04},
	journal = {arXiv:1612.08498 [cs, stat]},
	author = {Cohen, Taco S. and Welling, Max},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.08498},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Cohen and Welling - 2016 - Steerable CNNs.pdf:C\:\\wenhao\\zotero\\storage\\RRFJCIPP\\Cohen and Welling - 2016 - Steerable CNNs.pdf:application/pdf},
}

@article{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1602.07576},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
	urldate = {2022-02-04},
	journal = {arXiv:1602.07576 [cs, stat]},
	author = {Cohen, Taco S. and Welling, Max},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.07576},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Cohen and Welling - 2016 - Group Equivariant Convolutional Networks.pdf:C\:\\wenhao\\zotero\\storage\\XIDI95GR\\Cohen and Welling - 2016 - Group Equivariant Convolutional Networks.pdf:application/pdf},
}

@article{esteves_learning_2018,
	title = {Learning {SO}(3) {Equivariant} {Representations} with {Spherical} {CNNs}},
	url = {http://arxiv.org/abs/1711.06721},
	abstract = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multivalued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
	language = {en},
	urldate = {2022-02-04},
	journal = {arXiv:1711.06721 [cs]},
	author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
	month = sep,
	year = {2018},
	note = {arXiv: 1711.06721},
	keywords = {Convolution network, Spherical images},
	file = {Esteves et al. - 2018 - Learning SO(3) Equivariant Representations with Sp.pdf:C\:\\wenhao\\zotero\\storage\\3ESYCMPC\\Esteves et al. - 2018 - Learning SO(3) Equivariant Representations with Sp.pdf:application/pdf},
}

@article{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	language = {en},
	urldate = {2022-02-04},
	journal = {arXiv:1612.00593 [cs]},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv: 1612.00593},
	keywords = {Network on points},
	file = {Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:C\:\\wenhao\\zotero\\storage\\RGH2HU3Y\\Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:application/pdf},
}

@article{qi_pointnet_2017-1,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	language = {en},
	urldate = {2022-02-04},
	journal = {arXiv:1706.02413 [cs]},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02413},
	keywords = {Network on points},
	file = {Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on .pdf:C\:\\wenhao\\zotero\\storage\\SJCGGUGU\\Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on .pdf:application/pdf},
}

@book{dresselhaus_group_2008,
	address = {Berlin},
	title = {Group theory: application to the physics of condensed matter},
	isbn = {978-3-540-32897-1},
	shorttitle = {Group theory},
	publisher = {Springer-Verlag},
	author = {Dresselhaus, M. S. and Dresselhaus, G. and Jorio, A.},
	year = {2008},
	note = {OCLC: ocn150354198},
	keywords = {Condensed matter, Group theory},
}
