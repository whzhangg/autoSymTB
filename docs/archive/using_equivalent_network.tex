\documentclass{article}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{hyperref} % \url \href
\usepackage{docmute}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator{\spn}{Span}

\usepackage[style=chem-acs ,backend=bibtex, sorting=none]{biblatex}
\addbibresource{autoTB.bib}

\begin{document}

\section{METHODS}

\subsection{Deep Tensor Neural Network (DTNN)}
To utilize the symmetry relationship in the first way, we use the equivariant neural network
structure purposed by Thomas et al.\cite{thomas_tensor_2018} (Deep Tensor Neural Network). 
The network structure (DTNN) is reviewed in Appendix B. 
An important point that separate the equivariant network with general networks using 
invariant descriptors is that the transformation properties are kept in all layers of the equivariant
network such as one purposed by Thomas et al, but transformation properties are not kept if we 
use invariant descriptors. For example, the output of DTNN will change sign for a scalar output  
if a $\pi$ rotation is applied to an $p$ orbital, while the output will be the same for any
simple invariant network. This is one of the key property that we desire.
Furthermore, data in DTNN naturally flow in the form of tensor, which allow us to define 
tensor product, as in the case of Hamiltonian.

\subsection{Using the equivariant network}
Our network take inputs as two vectors and produce a scalar number. Since DTNN's output is 
also vector quantity, we define the network as $H(i,j)$ that operate on vector $| \psi_j \rangle$
\begin{equation}
    H |\psi\rangle = H(| \psi_i \rangle,| \psi_j \rangle) | \psi_j \rangle
\end{equation}
so that the tight-binding matrix element is naturally:
\begin{equation}
    H_{ij} = \langle \psi_i | H | \psi_j \rangle
\end{equation}
DTNN is rotational equivariant, meaning that we have: $H R | \psi_j \rangle  = RH | \psi_j \rangle $, 
as in a general equivariant network, 
where $R$ is an symmetry operation. 

let's now we consider how to use the above result to reduce the number of 
tight-binding matrix element to learn. 
We focus on nearest neighbor interaction and consider the above example, where
we place $p_x$ orbital at the center and two $s^1$, $s^2$ orbitals at $r_1 = (1,0,0)$ and $r_2 = (-1,0,0)$. 
How $s^1$ transform to $s^2$ is not unique, but we consider a $C_2$ rotation for concreteness. 
However, we do not include translation in our consideration but only the point group 
operations (vector transformation). 

Knowing the energy $\langle p_x | H | s^1 \rangle$, how do we derive the energy 
of $\langle p_x | H | s^2 \rangle$? Since $| s^2 \rangle = R | s^1 \rangle$ We have:
\begin{equation}
    \langle p_x | H | s^2 \rangle = \langle p_x | H R | s^1 \rangle
    = \langle p_x | R H | s^1 \rangle = \langle R^{-1} p_x | H | s^1 \rangle
\end{equation}
Since we know how rotation transform $p_x$, we recover the relationship 
\begin{equation}
    \langle p_x | H | s^2 \rangle = - \langle p_x | H | s^1 \rangle
\end{equation}

As a second example, let's consider $pp_{\sigma}$ bond, replacing $s^1$ with $p_x^1$ 
and $s^2$ with $-p_x^2$. Note the negative sign ensures that two orbitals at $r_1$ and $r_2$
are generated by point group symmetry. 
We have:
\begin{equation}
    - \langle p_x^0 | H | p_x^2 \rangle =  - \langle p_x^0 | HR | p_x^1 \rangle 
    = - \langle p_x^0 | RH | p_x^1 \rangle = \langle p_x^0 | H | p_x^1 \rangle
\end{equation} 
which is again the correct results. 


\end{document}
